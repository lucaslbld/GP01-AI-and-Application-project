# TrustThePixels: Machine Learning Techniques for Distinguishing AI-Generated Images from Real Photographs

**Members**  
- Thibault MICHEL, Computer science, Hanyang University, thibault.michel@edu.devinci.fr  
- Arthur LOURENÇO, Computer science, Hanyang University, arthur.lourenco@edu.ece.fr
- Jean LECORNET, Computer science, Hanyang University, jean.lecornet@edu.ece.fr
- Loup FLORENTIN, Computer science, Hanyang University, loup.florentin@edu.ece.fr
- Lucas LEBLOND, Computer science, Hanyang University, lucas.leblond@edu.ece.fr

---

## Table of Contents

1. [Introduction](#Introduction)  
2. [Datasets](#datasets)  
3. [Methodology](#methodology)  
4. [Evaluation & Analysis](#evaluation--analysis)  
5. [Related Work](#related-work)  
6. [Conclusion](#conclusion)  
7. [Video / Audio Link](#video--audio-link)

---

## Introduction

### Motivation
In recent years, the rapid development of generative AI systems such as ChatGPT, Midjourney, DALL·E, and more recently Sora has completely transformed the way digital content is created and shared. Access to these technologies has become extremely easy for the general public. While this accessibility brings many positive opportunities, it also raises serious concerns, especially regarding misinformation, content authenticity, and the spread of manipulated media.

Generative models have become so advanced that it is increasingly difficult for humans to distinguish AI-generated images from real photographs. On social networks, misleading or entirely fabricated content can circulate faster than verified information, amplifying the risk of confusion and intentional deception. As a result, it is now more important than ever to develop automated tools capable of identifying whether an image is real or produced by AI.

Such detection systems could eventually be integrated into social media platforms to automatically verify uploaded content and require AI-generated images to be clearly labeled. This would contribute to transparency, trust, and safer digital ecosystems.

### Project Goal
The objective of this project is to develop a machine learning model capable of determining whether an image is a real photograph or generated by artificial intelligence. To achieve this, we will build a complete end-to-end pipeline: collecting and describing the dataset, preprocessing and preparing the images, extracting relevant features, training classification models, and evaluating their performance. The aim is to explore how machine learning can support digital authenticity and contribute to future systems that automatically detect AI-generated visual content.

## Datasets
During the early stages of the project, we examined several datasets containing AI-generated images and real photographs. However, most available datasets presented two main problems.

First, many of them were extremely large, often containing hundreds of thousands of images. While these datasets are excellent for large-scale research, they are impractical for a student project because they require long download times, significant storage space, and substantial computing resources for both preprocessing and training.

Second, the smaller datasets we found were not suitable for our objectives. Many lacked diversity, included too few samples, or were limited to specific image categories such as human faces. Others suffered from poor labeling quality or contained images that did not align with the types of modern AI-generated content we wanted to study. Additionally, several datasets contained images with highly inconsistent resolutions, including extremely small files such as 32×32 or 64×64 pixels, which significantly increases noise during training and reduces the model’s ability to generalize.

Due to these limitations, we ultimately decided to use a large and diverse dataset from Hugging Face:
https://huggingface.co/datasets/theminji/ai-vs-real-200k

This dataset originally contains roughly 200,000 images divided into two categories: AI-generated content and real photographs. It covers a broad range of visual styles and subjects, making it highly relevant to our goal of detecting modern generative AI content.

However, using the full dataset was unnecessary for our project. To build a manageable and balanced working set, we wrote a Python script that loads the dataset, identifies the AI and Real labels automatically, randomly selects 5,000 AI-generated images and 5,000 real images, converts all images to RGB format for consistency, and saves them into two dedicated folders. The final result is a curated dataset of 10,000 images, perfectly balanced and much easier to work with for training and evaluation.

Below is the code used to create this reduced dataset.

    import os
    import random
    from datasets import load_dataset

    def main():
        ds = load_dataset("theminji/ai-vs-real-200k")
        train = ds["train"]

    print(train)
    print("Features:", train.features)

    label_field = "label"

    label_names = train.features[label_field].names
    print("Label names:", label_names)

  
    ai_label = None
    real_label = None

    for idx, name in enumerate(label_names):
        lname = name.lower()
        if "ai" in lname or "fake" in lname or "generated" in lname:
            ai_label = idx
        if "real" in lname:
            real_label = idx

    print("AI label:", ai_label, "(", label_names[ai_label], ")")
    print("Real label:", real_label, "(", label_names[real_label], ")")

    ai_indices = []
    real_indices = []

    for idx in range(len(train)):
        lbl = train[idx][label_field]
        if lbl == ai_label:
            ai_indices.append(idx)
        elif lbl == real_label:
            real_indices.append(idx)

    print("Total AI images:", len(ai_indices))
    print("Total Real images:", len(real_indices))

    random.seed(42)
    ai_sample = random.sample(ai_indices, 5000)
    real_sample = random.sample(real_indices, 5000)

    os.makedirs("dataset/AI", exist_ok=True)
    os.makedirs("dataset/Real", exist_ok=True)

    for i, idx in enumerate(ai_sample):
        img = train[idx]["image"]
        if img.mode != "RGB":
            img = img.convert("RGB")
        img.save(f"dataset/AI/ai_{i:04d}.jpg")

    for i, idx in enumerate(real_sample):
        img = train[idx]["image"]
        if img.mode != "RGB":
            img = img.convert("RGB")
        img.save(f"dataset/Real/real_{i:04d}.jpg")


    if __name__ == "__main__":
    main()


### Download the Dataset

Our dataset (10000 images)
[Click here to download the dataset](https://github.com/lucaslbld/GP01-AI-and-Application-project/releases/download/v1.0/dataset.zip)


## Methodology

The goal of this project is to train a machine learning model that can look at an image and decide whether it was created by a generative AI model or captured in the real world. In order to build such a system, we followed a step-by-step workflow that transforms the raw dataset into a trained model capable of making predictions.
The first step was to prepare a clean and balanced dataset. The original dataset contained around 200,000 images, which is far too large to use directly in our project. To keep things manageable while still preserving a high level of diversity, we wrote a Python script that randomly selects 5,000 AI-generated images and 5,000 real images from the full dataset. All selected images are converted into the same format (RGB) and stored in two folders named “AI” and “Real”. This results in a balanced dataset of 10,000 images that is much easier to work with during training and evaluation.
Once the dataset was ready, the second step was image preprocessing. Machine learning models cannot work directly with raw images, so each picture needs to be transformed in a consistent way. Every image is resized to 224×224 pixels (a standard size used by many image models), converted to a tensor, and normalized using standard ImageNet statistics. These transformations ensure that all images look “similar” from the model’s point of view, regardless of their original size, brightness, or color differences.
After preparing the images, the next step was model selection. Instead of building a neural network from scratch, which would require a very large dataset, we used a pre-trained model called ResNet34. This model has already been trained on millions of images and knows how to recognize general visual patterns such as edges, shapes, textures, shadows, and color structures. We only needed to “fine-tune” it for our specific task: distinguishing AI content from real photographs. To do this, we replaced the last layer of ResNet34 with a new layer that outputs two classes (AI vs Real).
The training process was carried out using PyTorch Lightning, which manages the training loop for us. The model goes through the images multiple times, in what we call “epochs”. During each epoch, the model makes predictions on the training images, compares its predictions with the correct labels, and adjusts its internal parameters to reduce its mistakes. This process gradually improves the model’s ability to recognize typical signs of AI-generated content or real-world photography. We also used a separate set of images (the validation set) to check how well the model performs on pictures it has never seen before.
Finally, after training, we saved the model and created two different prediction scripts. The first script performs a standard prediction by randomly selecting images from the prepared dataset and displaying the model’s predictions and confidence scores. This is useful for quickly checking how the model behaves on known data. The second script allows the user to choose and test any custom image, such as a photograph downloaded from the internet or taken with a phone. This script applies the same preprocessing steps, feeds the image into the trained model, and returns both the predicted label (AI or Real) and the associated probabilities. These two tools allow us to evaluate the model on controlled examples as well as real-world cases, and to better understand how well it generalizes beyond the original dataset.
Overall, this methodology provides a clear and reproducible workflow for training an AI-vs-Real image classifier. It also highlights the challenges involved in this task, such as variations in image quality, diversity of visual styles, and the increasing realism of modern generative models.

## Evaluation & Analysis

## Related Work


## Conclusion

## Video / Audio Link

